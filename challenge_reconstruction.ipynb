{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "#import warnings\n",
    "\n",
    "#from distutils.version import LooseVersion\n",
    "#import tensorflow as tf\n",
    "\n",
    "import numpy\n",
    "from numpy import load, save\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import os.path\n",
    "\n",
    "import densenet\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "#assert LooseVersion(tf.__version__) >= LooseVersion('1.0')\n",
    "#print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "#Check for a GPU\n",
    "#if not tf.test.gpu_device_name():\n",
    "#    warnings.warn('No GPU found')\n",
    "#else:\n",
    "#    print('Deafault GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "img_rows, img_cols = 64, 64\n",
    "img_height_test, img_width_test = 64, 64\n",
    "\n",
    "speckle_data = load('speckle_array_case0.npy')\n",
    "print(speckle_data.shape)\n",
    "#speckle_labels = load('speckle_labels.npy')\n",
    "speckle_labels = load('symbol_array_case0.npy')\n",
    "print(speckle_labels.shape)\n",
    "#plt.imshow(speckle_labels[2], cmap='gray')\n",
    "#plt.show()\n",
    "#dictionary = {speckle_labels_n: speckle_labels_mn_n for speckle_labels_n, speckle_labels_mn_n in zip(speckle_labels, speckle_labels_mn)}\n",
    "\n",
    "trainX, testX, Y_train, Y_test = train_test_split(speckle_data, speckle_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "trainX = trainX.reshape(-1, img_rows, img_cols, 1)\n",
    "testX = testX.reshape(-1, img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "Y_train = Y_train.reshape(-1, img_height_test, img_width_test, 1)\n",
    "Y_test = Y_test.reshape(-1, img_height_test, img_width_test, 1)\n",
    "input_shape_test = (img_height_test, img_width_test, 1)\n",
    "\n",
    "batch_size = 100\n",
    "nb_classes = 6\n",
    "nb_epoch = 5\n",
    "\n",
    "img_channels = 1\n",
    "\n",
    "img_dim = (img_channels, img_rows, img_cols) if K.image_dim_ordering() == \"th\" else (img_rows, img_cols, img_channels)\n",
    "print(img_dim)\n",
    "depth = 40\n",
    "nb_dense_block = 3\n",
    "growth_rate = 12\n",
    "nb_filter = -1\n",
    "dropout_rate = 0.0 # 0.0 for data augmentation\n",
    "\n",
    "# model = densenet.DenseNetFCN(input_shape=img_dim, depth=depth, include_top=True, nb_dense_block=nb_dense_block, classes=1, nb_filter=-1, \n",
    "#                             growth_rate=16, dropout_rate=dropout_rate)\n",
    "\n",
    "model = densenet.DenseNetFCN(input_shape=img_dim)\n",
    "\n",
    "print(\"Model created\")\n",
    "\n",
    "model.summary()\n",
    "optimizer = Adam(lr=1e-3) # Using Adam instead of SGD to speed up training\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "print(\"Finished compiling\")\n",
    "print(\"Building model...\")\n",
    "\n",
    "print('After preprocess_input: {}'.format(trainX.shape))\n",
    "print('After preprocess_input: {}'.format(testX.shape))\n",
    "\n",
    "model.fit(trainX, Y_train, \n",
    "          batch_size = 50, \n",
    "          epochs = 300, \n",
    "          verbose = 1, \n",
    "          validation_data = (testX, Y_test)) # Data on which to evaluate the loss and any model metrics at the end of each epoch. \n",
    "                                              # The model will not be trained on this data. \n",
    "                                              # This could be a list (x_val, y_val) or a list (x_val, y_val, val_sample_weights). \n",
    "                                              # validation_data will override validation_split.\n",
    "\n",
    "score = model.evaluate(testX, Y_test, verbose = 0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test acuracy:', score[1]) \n",
    "\n",
    "y_predicted = model.predict(testX)\n",
    "\n",
    "print(y_predicted.shape)\n",
    "save('challenge_predicted.npy', y_predicted)\n",
    "\n",
    "# extract = Model(reconstruction.inputs, reconstruction.layers[-1].output) # Dense(128,...)\n",
    "# features = extract.predict(testX)\n",
    "# print(features.shape)\n",
    "# save('features_data', features)\n",
    "# save('features_predicted', y_predicted)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------- ###\n",
    "###                  Original Code                                                                  ###\n",
    "\n",
    "# generator = ImageDataGenerator(rotation_range=15,\n",
    "#                                width_shift_range=5./32,\n",
    "#                                height_shift_range=5./32,\n",
    "#                                horizontal_flip=True)\n",
    "\n",
    "# generator.fit(trainX, seed=0)\n",
    "\n",
    "# # Load model\n",
    "# weights_file=\"weights/DenseNet-40-12-CIFAR10.h5\"\n",
    "# if os.path.exists(weights_file):\n",
    "#     #model.load_weights(weights_file, by_name=True)\n",
    "#     print(\"Model loaded.\")\n",
    "\n",
    "# out_dir=\"weights/\"\n",
    "\n",
    "# lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
    "#                                     cooldown=0, patience=5, min_lr=1e-5)\n",
    "# model_checkpoint= ModelCheckpoint(weights_file, monitor=\"val_acc\", save_best_only=True,\n",
    "#                                   save_weights_only=True, verbose=1)\n",
    "\n",
    "# callbacks=[lr_reducer, model_checkpoint]\n",
    "\n",
    "# model.fit_generator(generator.flow(trainX, Y_train, batch_size=batch_size),\n",
    "#                     steps_per_epoch=len(trainX) // batch_size, epochs=nb_epoch,\n",
    "#                     # callbacks=callbacks,\n",
    "#                     validation_data=(testX, Y_test),\n",
    "#                     validation_steps=testX.shape[0] // batch_size, verbose=1)\n",
    "\n",
    "# yPreds = model.predict(testX)\n",
    "# print(yPreds.shape)\n",
    "# save('challenge_predicted.npy', yPreds)\n",
    "# yPred = np.argmax(yPreds, axis=1)\n",
    "# yTrue = testY\n",
    "\n",
    "# accuracy = metrics.accuracy_score(yTrue, yPred) * 100\n",
    "# error = 100 - accuracy\n",
    "# print(\"Accuracy : \", accuracy)\n",
    "# print(\"Error : \", error)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
